{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35b42ff6",
   "metadata": {},
   "source": [
    "# Chapter 5: LangChain Introduction\n",
    "\n",
    "These examples demonstrate various capabilities of LangChain.\n",
    "\n",
    "Some of these capabilities did not work well with either small Ollama models (running on the CPU) or the Ollama Open AI compatible API (for example, the tools feature).\n",
    "\n",
    "Therefore, I ran the examples in this file are run with the Anthropic Open AI compatible API, running `claude-3-5-haiku-20241022`\n",
    "\n",
    "### Env File Setup\n",
    "\n",
    "To provide environment variables to this Jupyter noteboko, the `dotenv` package is used. Create a file in the `chapter5/python` directory called `.env`, paste in these values, and upload them with your own. If you modify the file, just restart your Jupyter kernel and execute the first few cells again.\n",
    "\n",
    "```env\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_TRACING_V2=true\n",
    "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
    "LANGSMITH_API_KEY=\"<your-api-key-here>\"\n",
    "LANGSMITH_PROJECT=\"<your-project-name-here>\"\n",
    "# If you don't have a MODEL_NAME, the llama3.2 model is used.\n",
    "# You can comment them out to easily switch between them.\n",
    "# MODEL_NAME=\"qwen2.5:3b\"\n",
    "# MODEL_NAME=\"gemma3:12b\"\n",
    "OPENAI_API_KEY=\"<your-api-key-here>\"\n",
    "\n",
    "# To use the Anthropic API, use this\n",
    "#OPENAI_BASE_URL=\"https://api.anthropic.com/v1/\"\n",
    "#MODEL_NAME=\"claude-3-5-haiku-20241022\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5bf1a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage, SystemMessage \n",
    "from pydantic import validate_call\n",
    "\n",
    "base_url = os.environ.get(\"OPENAI_BASE_URL\") or \"http://localhost:11434/v1\"\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\") or \"ollama\"\n",
    "model_name = os.environ.get(\"MODEL_NAME\") or (\"llama3.2\" if api_key == \"ollama\" else \"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1bd263",
   "metadata": {},
   "source": [
    "### Create and test a langchain OpenAI client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f24b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How are you doing today? Is there anything I can help you with?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 20, 'prompt_tokens': 9, 'total_tokens': 29, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'claude-3-5-haiku-20241022', 'system_fingerprint': None, 'id': 'msg_014hFa7EgNE7QXenqw5YtKZX', 'finish_reason': 'stop', 'logprobs': None}, id='run-24b15899-13c2-4ccd-a4b0-ccac38f747d0-0', usage_metadata={'input_tokens': 9, 'output_tokens': 20, 'total_tokens': 29, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def get_model():\n",
    "    \"\"\"Call this method to get a new model if you want one for some reason\"\"\"\n",
    "    return ChatOpenAI(\n",
    "        temperature=0.0,\n",
    "        model_name=model_name,\n",
    "        openai_api_key=api_key,\n",
    "        openai_api_base=base_url)\n",
    "\n",
    "chat_model = get_model()\n",
    "\n",
    "display(chat_model.invoke(\"hi!\"))\n",
    "\n",
    "text = \"I am looking to book a direct flight from New York to London departing on December 10th and returning on January 5th. Can you provide me with the available options, including airlines, flight times, and prices\" \n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You're a helpful flight expert\"),\n",
    "    HumanMessage(content=text),\n",
    "]\n",
    "\n",
    "# display(chat_model.invoke(messages))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a28586a",
   "metadata": {},
   "source": [
    "### Example of formatting a prompt template with interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c13260f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Look at the following conversation Customer: My new bike is missing a wheel! Chatbot: I'm sorry to hear that. Could I have your order number, please? Customer: It's #54321. Chatbot: Thank you. We'll send a replacement wheel today, and it'll arrive in two days. Customer: Make sure it does. This has been a hassle. Chatbot: Understandably so, and we apologize. You’ll also get a 20% discount on your next order for the inconvenience. Customer: Fine, thank you. Chatbot: You're welcome, and the confirmation is on its way. If there’s more I can do for you, just let me know. from the following service area complaints on 2023-10-19 14:30:00 and return a sentiment\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\n",
    "    \"Look at the following conversation {conversation} from the following service area {service_area} on {event_date_time} and return a sentiment\"\n",
    ")\n",
    "\n",
    "conversation = \"\"\"Customer: My new bike is missing a wheel! \\\n",
    "Chatbot: I'm sorry to hear that. Could I have your order number, please? \\\n",
    "Customer: It's #54321. \\\n",
    "Chatbot: Thank you. We'll send a replacement wheel today, and it'll arrive in two days. \\\n",
    "Customer: Make sure it does. This has been a hassle. \\\n",
    "Chatbot: Understandably so, and we apologize. You’ll also get a 20% discount on your next order for the inconvenience. \\\n",
    "Customer: Fine, thank you. \\\n",
    "Chatbot: You're welcome, and the confirmation is on its way. If there’s more I can do for you, just let me know.\"\"\"\n",
    "\n",
    "reminder_message = prompt_template.format(\n",
    "    conversation=conversation,\n",
    "    service_area=\"complaints\",\n",
    "    event_date_time=\"2023-10-19 14:30:00\"\n",
    ")\n",
    "reminder_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853cf89f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a health advisory bot for HealthHub Clinic. You can answer questions from the patient called Lucy', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hi, Lucy please ask me your question.', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='What are the symptoms of the flu?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Lucy, the flu (influenza) is a contagious respiratory illness caused by the influenza virus. The symptoms of the flu can vary from person to person, but common symptoms include:\\n\\n* Fever, usually high\\n* Chills\\n* Cough\\n* Sore throat\\n* Runny or stuffy nose\\n* Headache\\n* Fatigue (extreme tiredness)\\n* Muscle or body aches\\n* Diarrhea and vomiting (more common in children)\\n\\nIn some cases, the flu can also cause more severe symptoms, such as:\\n\\n* Pneumonia (infection of the lungs)\\n* Bronchitis (inflammation of the airways)\\n* Sinus and ear infections\\n\\nIf you're experiencing any of these symptoms, it's essential to consult with a healthcare professional for proper diagnosis and treatment.\\n\\nAt HealthHub Clinic, we can also provide you with guidance on how to manage your symptoms and prevent complications. Would you like me to recommend some over-the-counter medications or home remedies, Lucy?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 209, 'prompt_tokens': 67, 'total_tokens': 276, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'llama3.2', 'system_fingerprint': 'fp_ollama', 'id': 'chatcmpl-377', 'finish_reason': 'stop', 'logprobs': None}, id='run-30a0961a-d4dc-4dfc-87e0-6857dde7395e-0', usage_metadata={'input_tokens': 67, 'output_tokens': 209, 'total_tokens': 276, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate \n",
    "\n",
    "chat_template = ChatPromptTemplate.from_messages([ \n",
    "    (\"system\", \"You are a health advisory bot for HealthHub Clinic. You can answer questions from the patient called {name}\"), \n",
    "    (\"ai\", \"Hi, {name} please ask me your question.\"), \n",
    "    (\"human\", \"{user_input}\"), \n",
    "])\n",
    "\n",
    "messages = chat_template.format_messages(name=\"Lucy\", user_input=\"What are the symptoms of the flu?\")\n",
    "\n",
    "display(messages)\n",
    "\n",
    "chat_model.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286ee2a4",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate example\n",
    "\n",
    "Used to format a chat prompt (system, user and ai parts). This example creates a subclass to apply Pydantic validation to the template placeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d9947ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/zg1dfhgn6r17gm0mlw6fg3yc0000gn/T/ipykernel_71239/1927113081.py:5: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"input_variables\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "class ProductivityChatPromptTemplate(ChatPromptTemplate, BaseModel):\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        required_vars = {\"task\", \"time_available\", \"user_preferences\"}\n",
    "        if not required_vars.issubset(v):\n",
    "            raise ValueError(f\"Input variables must include: {required_vars}\")\n",
    "        return v\n",
    "\n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        messages = [\n",
    "            (\"system\", \"You are a virtual productivity assistant.\"),\n",
    "            (\"human\", f\"I need to {kwargs['task']} and I only have {kwargs['time_available']}.\"),\n",
    "            (\"human\", f\"My preference is to {kwargs['user_preferences']}.\"),\n",
    "            (\"ai\", \"Based on your task and preferences, here's my advice:\")\n",
    "            # The AI's response would be generated by the language model following this prompt.\n",
    "        ]\n",
    "        return self.construct_chat(messages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f05c9ce",
   "metadata": {},
   "source": [
    "### StringPromptTemplate example\n",
    "\n",
    "Used to format a basic string. This example creates a subclass to apply Pydantic validation to the template placeholders.\n",
    "\n",
    "The example also shows how to use Python reflection to get function definitions and incorporate them into the chat, hypothetically for creating a tool to help explain your code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "482058ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "\n",
    "def get_source_code(function_name):\n",
    "    # Get the source code of the function\n",
    "    return inspect.getsource(function_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46d53704",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/zg1dfhgn6r17gm0mlw6fg3yc0000gn/T/ipykernel_71239/1546726354.py:15: PydanticDeprecatedSince20: Pydantic V1 style `@validator` validators are deprecated. You should migrate to Pydantic V2 style `@field_validator` validators, see the migration guide for more details. Deprecated in Pydantic V2.0 to be removed in V3.0. See Pydantic V2 Migration Guide at https://errors.pydantic.dev/2.10/migration/\n",
      "  @validator(\"input_variables\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the function name and source code, generate an English language explanation of the function.\n",
      "Function Name: get_source_code\n",
      "Source Code:\n",
      "def get_source_code(function_name):\n",
      "    # Get the source code of the function\n",
      "    return inspect.getsource(function_name)\n",
      "\n",
      "Explanation:\n",
      "\n",
      "**Function Explanation: `get_source_code`**\n",
      "\n",
      "The `get_source_code` function is designed to retrieve the source code of a given Python function. It takes one argument, `function_name`, which is the name of the function for which you want to obtain its source code.\n",
      "\n",
      "Here's how it works:\n",
      "\n",
      "1. The function uses the built-in `inspect` module, which provides functions to help get information about live objects such as modules, classes, methods, etc.\n",
      "2. Specifically, it calls the `getsource()` function from the `inspect` module, passing in the `function_name` argument.\n",
      "3. The `getsource()` function returns the source code of the specified function as a string.\n",
      "\n",
      "In essence, this function allows you to programmatically access and inspect the source code of any Python function, making it easier to understand how functions are implemented and used in your codebase.\n",
      "\n",
      "**Example Use Case:**\n",
      "\n",
      "```python\n",
      "import inspect\n",
      "\n",
      "def my_function():\n",
      "    print(\"Hello, World!\")\n",
      "\n",
      "source_code = get_source_code(my_function)\n",
      "print(source_code)\n",
      "```\n",
      "\n",
      "This example would output the source code of `my_function`, which is then printed to the console.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import StringPromptTemplate\n",
    "from pydantic import BaseModel, validator\n",
    "\n",
    "PROMPT = \"\"\"\\\n",
    "Given the function name and source code, generate an English language explanation of the function.\n",
    "Function Name: {function_name}\n",
    "Source Code:\n",
    "{source_code}\n",
    "Explanation:\n",
    "\"\"\"\n",
    "\n",
    "class FunctionExplainerPromptTemplate(StringPromptTemplate, BaseModel):\n",
    "    \"\"\"A custom prompt template that takes in the function name as input, and formats the prompt template to provide the source code of the function.\"\"\"\n",
    "\n",
    "    @validator(\"input_variables\")\n",
    "    def validate_input_variables(cls, v):\n",
    "        \"\"\"Validate that the input variables are correct.\"\"\"\n",
    "        if len(v) != 1 or \"function_name\" not in v:\n",
    "            raise ValueError(\"function_name must be the only input_variable.\")\n",
    "        return v\n",
    "\n",
    "    def format(self, **kwargs) -> str:\n",
    "        # Get the source code of the function\n",
    "        source_code = get_source_code(kwargs[\"function_name\"])\n",
    "\n",
    "        # Generate the prompt to be sent to the language model\n",
    "        prompt = PROMPT.format(\n",
    "            function_name=kwargs[\"function_name\"].__name__, source_code=source_code\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def _prompt_type(self):\n",
    "        return \"function-explainer\"\n",
    "    \n",
    "    \n",
    "fn_explainer = FunctionExplainerPromptTemplate(input_variables=[\"function_name\"])\n",
    "\n",
    "# Generate a prompt for the function \"get_source_code\"\n",
    "prompt = fn_explainer.format(function_name=get_source_code)\n",
    "print(prompt)\n",
    "\n",
    "print(chat_model.invoke(prompt).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63d2efec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
      "\n",
      "```json\n",
      "{\n",
      "\t\"intents\": string  // Format the output as a single JSON object consisting of a key: intents, the intents key will be a list of objects with the following keys: intent, utterance, category\n",
      "}\n",
      "```\n",
      "Got response\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='```json\\n{\\n    \"intents\": [\\n        {\\n            \"intent\": \"facilities_library_info\",\\n            \"category\": \"facilities\",\\n            \"utterances\": [\\n                \"What are the library\\'s operating hours?\",\\n                \"Can I access online resources from the library?\",\\n                \"Tell me about the study spaces and resources available in the college library\",\\n                \"Do you have information about the library\\'s digital archives and research databases?\"\\n            ]\\n        },\\n        {\\n            \"intent\": \"facilities_campus_services\",\\n            \"category\": \"facilities\", \\n            \"utterances\": [\\n                \"Where is the student health center located?\",\\n                \"What counseling services are available on campus?\",\\n                \"Can you provide details about disability support services and accommodations for students with special needs\",\\n                \"Tell me about the campus wellness and mental health resources\"\\n            ]\\n        },\\n        {\\n            \"intent\": \"course_information_programs\",\\n            \"category\": \"course_information\",\\n            \"utterances\": [\\n                \"What majors do you offer?\",\\n                \"Can I see a list of undergraduate programs?\",\\n                \"I\\'m interested in learning about the interdisciplinary degree programs and unique academic tracks available at the college\",\\n                \"Provide comprehensive information about the academic departments and specialized study options\"\\n            ]\\n        },\\n        {\\n            \"intent\": \"course_information_enrollment\",\\n            \"category\": \"course_information\", \\n            \"utterances\": [\\n                \"How do I register for classes?\",\\n                \"When is the course registration deadline?\",\\n                \"Can you explain the detailed process for course selection, prerequisite requirements, and academic advising for new students\",\\n                \"Provide a comprehensive overview of the course enrollment process, including add/drop periods and academic planning strategies\"\\n            ]\\n        }\\n    ]\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 157, 'total_tokens': 581, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'claude-3-5-haiku-20241022', 'system_fingerprint': None, 'id': 'msg_015m1nbk2N6ivNDyeiR6ZCA9', 'finish_reason': 'stop', 'logprobs': None}, id='run-6961f551-d59e-4111-8f34-0d0d669372b2-0', usage_metadata={'input_tokens': 157, 'output_tokens': 424, 'total_tokens': 581, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'intent': 'facilities_library_info', 'category': 'facilities', 'utterances': [\"What are the library's operating hours?\", 'Can I access online resources from the library?', 'Tell me about the study spaces and resources available in the college library', \"Do you have information about the library's digital archives and research databases?\"]}, {'intent': 'facilities_campus_services', 'category': 'facilities', 'utterances': ['Where is the student health center located?', 'What counseling services are available on campus?', 'Can you provide details about disability support services and accommodations for students with special needs', 'Tell me about the campus wellness and mental health resources']}, {'intent': 'course_information_programs', 'category': 'course_information', 'utterances': ['What majors do you offer?', 'Can I see a list of undergraduate programs?', \"I'm interested in learning about the interdisciplinary degree programs and unique academic tracks available at the college\", 'Provide comprehensive information about the academic departments and specialized study options']}, {'intent': 'course_information_enrollment', 'category': 'course_information', 'utterances': ['How do I register for classes?', 'When is the course registration deadline?', 'Can you explain the detailed process for course selection, prerequisite requirements, and academic advising for new students', 'Provide a comprehensive overview of the course enrollment process, including add/drop periods and academic planning strategies']}]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'intent': 'facilities_library_info',\n",
       "  'category': 'facilities',\n",
       "  'utterances': [\"What are the library's operating hours?\",\n",
       "   'Can I access online resources from the library?',\n",
       "   'Tell me about the study spaces and resources available in the college library',\n",
       "   \"Do you have information about the library's digital archives and research databases?\"]},\n",
       " {'intent': 'facilities_campus_services',\n",
       "  'category': 'facilities',\n",
       "  'utterances': ['Where is the student health center located?',\n",
       "   'What counseling services are available on campus?',\n",
       "   'Can you provide details about disability support services and accommodations for students with special needs',\n",
       "   'Tell me about the campus wellness and mental health resources']},\n",
       " {'intent': 'course_information_programs',\n",
       "  'category': 'course_information',\n",
       "  'utterances': ['What majors do you offer?',\n",
       "   'Can I see a list of undergraduate programs?',\n",
       "   \"I'm interested in learning about the interdisciplinary degree programs and unique academic tracks available at the college\",\n",
       "   'Provide comprehensive information about the academic departments and specialized study options']},\n",
       " {'intent': 'course_information_enrollment',\n",
       "  'category': 'course_information',\n",
       "  'utterances': ['How do I register for classes?',\n",
       "   'When is the course registration deadline?',\n",
       "   'Can you explain the detailed process for course selection, prerequisite requirements, and academic advising for new students',\n",
       "   'Provide a comprehensive overview of the course enrollment process, including add/drop periods and academic planning strategies']}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "intent_schema = ResponseSchema(name=\"intents\",\n",
    "                             description=\"Format the output as a single JSON object consisting of a key: intents, the intents key will be a list of objects with the following keys: intent, utterance, category\")\n",
    "response_schemas = [intent_schema]\n",
    "\n",
    "intent_output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "intent_format_instructions = intent_output_parser.get_format_instructions()\n",
    "\n",
    "print(intent_format_instructions)\n",
    "\n",
    "intent_template = \"\"\"\n",
    "Create intents and utterances for a chatbot which will answer questions about a college, \\\n",
    "Create 4 examples of intents for the following categories: facilities and course_information. \\\n",
    "Ensure that each intent has 4 utterances total, including 2 long tail and 2 more common utterances.\n",
    "\n",
    "{intent_format_instructions}\n",
    "\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(intent_template)\n",
    "                                                           \n",
    "messages = prompt_template.format_messages(intent_format_instructions=intent_format_instructions)\n",
    "#messages = prompt_template.format_messages(intent_examples=intents)\n",
    "response = chat_model.invoke(messages)\n",
    "print(\"Got response\")\n",
    "# this will be type str\n",
    "print(type(response.content))\n",
    "display(response)\n",
    "                               \n",
    "output_dict = intent_output_parser.parse(response.content)\n",
    "print(output_dict)\n",
    "output_dict.get('intents')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998ca08a",
   "metadata": {},
   "source": [
    "### LCEL Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fb6fff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Quelle est la capitale de l'Angleterre ?\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "    Translate this text, without saying anything else: {text} to {language} \n",
    "\"\"\"\n",
    ")\n",
    "language = \"French\"\n",
    "text = \"what is the capital of england\"\n",
    "\n",
    "chain = prompt | chat_model | StrOutputParser()\n",
    "chain.invoke({\"text\": text, \"language\": language})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab1bc6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'transactional_intents': [{'category': 'maintenance_scheduling',\n",
       "   'intent': 'service_booking',\n",
       "   'live_chat_transcript': [{'role': 'customer',\n",
       "     'message': 'Hello, I need to schedule a maintenance appointment for my ElectricZ Model 3. Can you help with that?'},\n",
       "    {'role': 'support_agent',\n",
       "     'message': \"Certainly! Can you please provide your car's VIN and let me know what type of maintenance you require?\"},\n",
       "    {'role': 'customer',\n",
       "     'message': 'My VIN is XYZ12345, and I need a routine checkup.'},\n",
       "    {'role': 'support_agent',\n",
       "     'message': \"Thank you! I've scheduled your maintenance appointment. You'll receive a confirmation email shortly. Is there anything else I can assist you with?\"}]},\n",
       "  {'category': 'subscription_change',\n",
       "   'intent': 'product_upgrade',\n",
       "   'live_chat_transcript': [{'role': 'customer',\n",
       "     'message': \"Hello, I'd like to change my subscription plan from the ElectricA Model Y to the ElectricB Model Z. How can I do that?\"},\n",
       "    {'role': 'support_agent',\n",
       "     'message': \"Certainly! To change your subscription plan, please send us an email with your request, and we'll assist you with the process.\"},\n",
       "    {'role': 'customer',\n",
       "     'message': \"Thank you! I'll send the email right away.\"}]},\n",
       "  {'category': 'billing_inquiry',\n",
       "   'intent': 'billing_information',\n",
       "   'live_chat_transcript': [{'role': 'customer',\n",
       "     'message': 'Hello, can you explain how billing works for the subscription?'},\n",
       "    {'role': 'support_agent',\n",
       "     'message': \"Of course! We bill you on a monthly basis, and the amount is based on your chosen subscription plan. You'll receive an invoice via email. Do you have any specific billing questions?\"},\n",
       "    {'role': 'customer',\n",
       "     'message': 'No, I just wanted to understand the billing process. Thanks for the explanation!'}]}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filename: Sequential chain transcript processor \n",
    "import json\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import SystemMessagePromptTemplate, HumanMessagePromptTemplate, ChatPromptTemplate\n",
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# Load transcripts data\n",
    "with open('transcripts.json', 'r') as file:\n",
    "    transcripts = json.load(file)\n",
    "\n",
    "transcripts[\"conversations\"] = transcripts[\"conversations\"][:5]\n",
    "\n",
    "# Define response schemas\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"transactional_transcripts\", description=\"Format the output as JSON list of conversations with the same JSON format as the input,add an category key to each conversation. Remember that JSON strings use\\\"\", type=\"list\"),\n",
    "    ResponseSchema(name=\"faq\", description=\"Format the output as JSON list of conversations with the same JSON format as the input, add an category key to each conversation. Remember that JSON strings use\\\"\", type=\"list\"),\n",
    "]\n",
    "\n",
    "# Define chat transcript template with placeholders for transcripts and format instructions\n",
    "transcript_template = \"Look at the following chat transcripts {transcripts} and categorize them into FAQ and transactional conversations in the following format {format_instructions}\"\n",
    "\n",
    "\n",
    "# Define prompt templates\n",
    "transactional_categorization_prompt_template = HumanMessagePromptTemplate.from_template(transcript_template)\n",
    "\n",
    "# Create output parser\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# Get format instructions\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "\n",
    "# Create prompts\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[transactional_categorization_prompt_template],\n",
    "    input_variables=[\"transcripts\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions},\n",
    ")\n",
    "\n",
    "# Define chain for transactional categorization\n",
    "chain_one = prompt | chat_model | output_parser\n",
    "\n",
    "#chain_one_result = chain_one.invoke({\"transcripts\": transcripts})\n",
    "\n",
    "# Define intent response schemas\n",
    "intent_response_schemas = [\n",
    "    ResponseSchema(name=\"transactional_intents\", description=\"Format the output as JSON list of conversation transcripts using the same format from the transactional_transcript list, add an intent key to each conversation\", type=\"list\"),\n",
    "]\n",
    "\n",
    "intent_transcript_template = \"Look at the following chat transcripts {transactional_transcripts} Cluster these conversations by intent {intent_format_instructions}\"\n",
    "\n",
    "\n",
    "# Create intent prompt\n",
    "intent_clustering_prompt_template = HumanMessagePromptTemplate.from_template(intent_transcript_template)\n",
    "\n",
    "# Create intent output parser\n",
    "intent_output_parser = StructuredOutputParser.from_response_schemas(intent_response_schemas)\n",
    "\n",
    "# Get intent format instructions\n",
    "intent_format_instructions = intent_output_parser.get_format_instructions()\n",
    "\n",
    "# Create prompt for intent clustering\n",
    "prompt_two = ChatPromptTemplate(\n",
    "    messages=[intent_clustering_prompt_template],\n",
    "    input_variables=[\"transactional_transcripts\"],\n",
    "    partial_variables={\"intent_format_instructions\": intent_format_instructions},\n",
    ")\n",
    "\n",
    "# Create chain for intent clustering\n",
    "chain2 = (\n",
    "    {\"transactional_transcripts\": chain_one}\n",
    "    | prompt_two\n",
    "    | chat_model\n",
    "    | intent_output_parser\n",
    ")\n",
    "\n",
    "# Pass transcripts through chain inputs\n",
    "chain_two_result = chain2.invoke({\"transcripts\": transcripts})\n",
    "chain_two_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a71587f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['transactional_intents'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_two_result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "993912c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a comprehensive summary of economic policy based on the provided sources:\n",
      "\n",
      "Economic Policy Overview:\n",
      "\n",
      "Definition:\n",
      "Economic policy consists of strategies and actions implemented by governments and central banks to influence a nation's economic conditions, growth, and financial well-being.\n",
      "\n",
      "Key Objectives:\n",
      "1. Economic growth\n",
      "2. Price stability\n",
      "3. Full employment\n",
      "4. Sustainable development\n",
      "5. Income distribution\n",
      "\n",
      "Main Types of Economic Policy:\n",
      "\n",
      "1. Fiscal Policy\n",
      "- Government spending and taxation strategies\n",
      "- Manages economic performance through budgets, tax rates, and public spending\n",
      "\n",
      "2. Monetary Policy\n",
      "- Controlled by central banks\n",
      "- Manages money supply and interest rates\n",
      "- Uses tools like interest rate adjustments and quantitative easing\n",
      "\n",
      "3. Trade Policy\n",
      "- Regulates international trade\n",
      "- Focuses on import/export regulations and trade agreements\n",
      "\n",
      "4. Industrial Policy\n",
      "- Supports specific industries\n",
      "- Provides subsidies and investment incentives\n",
      "\n",
      "5. Structural Policy\n",
      "- Addresses long-term economic transformation\n",
      "- Involves labor market regulations, education, and infrastructure development\n",
      "\n",
      "Recent Research Trends:\n",
      "\n",
      "1. Digital Currencies\n",
      "- Exploring central bank digital currencies (CBDCs)\n",
      "- Analyzing potential impacts on monetary policy\n",
      "\n",
      "2. Climate Change Integration\n",
      "- Developing economic policies that address environmental challenges\n",
      "- Exploring carbon pricing mechanisms\n",
      "\n",
      "3. Post-Pandemic Recovery\n",
      "- Investigating national economic recovery strategies\n",
      "- Comparing stimulus package effectiveness\n",
      "\n",
      "4. Technological Innovation\n",
      "- Creating policy mechanisms to support emerging technologies\n",
      "- Developing tax incentives and regulatory frameworks\n",
      "\n",
      "Contemporary Challenges:\n",
      "- Balancing competing economic goals\n",
      "- Managing economic cycles\n",
      "- Addressing income inequality\n",
      "- Responding to global economic shifts\n",
      "- Integrating sustainable development\n",
      "- Adapting to digital economy transformations\n",
      "\n",
      "Theoretical Approaches:\n",
      "- Keynesian Economics: Advocates government intervention\n",
      "- Neoclassical Economics: Promotes free-market principles\n",
      "- Monetarist Approach: Emphasizes money supply control\n",
      "\n",
      "Evaluation Metrics:\n",
      "- GDP growth\n",
      "- Inflation rate\n",
      "- Unemployment rate\n",
      "- Income distribution\n",
      "- Economic productivity\n",
      "\n",
      "Recommendation:\n",
      "For the most current information, consult:\n",
      "- Reputable news sources\n",
      "- Government websites\n",
      "- Academic economic journals\n",
      "- International economic organization publications\n"
     ]
    }
   ],
   "source": [
    "#filename: Parallel Chains in Langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableParallel\n",
    "from langchain.schema import StrOutputParser\n",
    "\n",
    "# prompt for querying news articles about a topic\n",
    "news_prompt = ChatPromptTemplate.from_template(\"summarize recent news articles about {topic}\")\n",
    "\n",
    "# Chain for querying scientific papers about a topic\n",
    "academic_prompt = ChatPromptTemplate.from_template(\"summarize recent scientific papers about {topic}\")\n",
    "\n",
    "# Chain for querying general web information about a topic\n",
    "web_info_prompt = ChatPromptTemplate.from_template(\"provide a general overview of {topic} from web sources\")\n",
    "\n",
    "\n",
    "# Create a RunnableParallel instance with the three chains\n",
    "# parallel_chain = RunnableParallel(news=news_chain, academic=academic_chain, web_info=web_info_chain)\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    news = news_prompt | chat_model,\n",
    "    academic = academic_prompt | chat_model,\n",
    "    web_info = web_info_prompt | chat_model\n",
    ")\n",
    "\n",
    "summarise_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "summarize the following information from these different sources:\n",
    "\n",
    "News source: {news}\n",
    "Academic: {academic}\n",
    "Web: {web_info}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "\n",
    "# Invoke the parallel chain with a specific topic\n",
    "#results = parallel_chain.invoke({\"topic\": \"artificial intelligence\"})\n",
    "\n",
    "summarise_chain = parallel | summarise_prompt | chat_model\n",
    "\n",
    "summarise_output = summarise_chain.invoke({\"topic\": \"Economic policy\"})\n",
    "\n",
    "print(summarise_output.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3067a13d",
   "metadata": {},
   "source": [
    "### Demonstration of RunnableParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfd975c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Here\\'s a summary of the joke and poem:\\n\\nJoke Summary:\\nA patient tells his doctor that he thinks he\\'s becoming a hypochondriac. The doctor humorously responds that the patient shouldn\\'t worry, as being a hypochondriac is \"nothing to be sick about\" - a playful wordplay that pokes fun at the irony of worrying about being a worrier.\\n\\nPoem Summary:\\nThe poem celebrates doctors as compassionate and knowledgeable professionals. It portrays them as healers with skilled hands, deep medical knowledge, and caring hearts. The poem emphasizes doctors as \"guardians of life\" who combine scientific expertise with hope, highlighting their critical role in healthcare and human well-being.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 601, 'total_tokens': 758, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'claude-3-5-haiku-20241022', 'system_fingerprint': None, 'id': 'msg_016srCfhwpFkGrRRFSnQLA3V', 'finish_reason': 'stop', 'logprobs': None} id='run-eeb66a70-6592-44ba-938a-ce6ee5fc6c60-0' usage_metadata={'input_tokens': 601, 'output_tokens': 157, 'total_tokens': 758, 'input_token_details': {}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate  \n",
    "from langchain.schema.runnable import RunnableParallel, RunnableSequence\n",
    "\n",
    "joke_prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "poem_prompt = ChatPromptTemplate.from_template(\"write a 2-line poem about {topic}\")\n",
    "\n",
    "parallel = RunnableParallel(\n",
    "    joke = joke_prompt | chat_model,\n",
    "    poem = poem_prompt | chat_model\n",
    ")\n",
    "\n",
    "summary_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Summarize the joke and poem about\n",
    "\n",
    "Joke: {joke}\n",
    "Poem: {poem}\n",
    "\n",
    "Summary:\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "sequence = parallel | summary_prompt | chat_model\n",
    "\n",
    "output = sequence.invoke({\"topic\": \"doctors\"})\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add986c8",
   "metadata": {},
   "source": [
    "### Example of classifying input and routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df300abc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m9/zg1dfhgn6r17gm0mlw6fg3yc0000gn/T/ipykernel_77851/1703045430.py:58: LangChainDeprecationWarning: The function `_convert_pydantic_to_openai_function` was deprecated in LangChain 0.1.16 and will be removed in 1.0. Use :meth:`~langchain_core.utils.function_calling.convert_to_openai_function()` instead.\n",
      "  classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableBranch\n",
    "from langchain.prompts import PromptTemplate\n",
    "from typing import Literal\n",
    "from langchain.output_parsers.openai_functions import PydanticAttrOutputFunctionsParser\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from operator import itemgetter\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from pydantic import BaseModel\n",
    "\n",
    "# Maintenance Department Template\n",
    "maintenance_template = \"\"\"You are an assistant at a car service center. \\\n",
    "You help customers book their cars for service. You will collect details such as the customer's name, \\\n",
    "car registration number, and their preferred date and time for the car collection.\n",
    "\n",
    "Here is the service booking request:\n",
    "{input}\"\"\"\n",
    "maintenance_prompt = PromptTemplate.from_template(maintenance_template)\n",
    "\n",
    "# Car Information Department Template\n",
    "car_info_template = \"\"\"You are knowledgeable about various car models and their features. \\\n",
    "You can provide detailed information about car specifications, models, and performance. \\\n",
    "If a question is outside your expertise, you recommend contacting the car manufacturer.\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "car_info_prompt = PromptTemplate.from_template(car_info_template)\n",
    "\n",
    "# Accounts Department Template\n",
    "accounts_template = \"\"\"You are well-versed in account management for car subscriptions. \\\n",
    "You can answer questions about billing, payment methods, and subscription plans. \\\n",
    "In cases of specific account issues, you advise contacting the accounts department directly.\n",
    "\n",
    "Here is the question:\n",
    "{input}\"\"\"\n",
    "accounts_prompt = PromptTemplate.from_template(accounts_template)\n",
    "\n",
    "# General Prompt Template for Other Queries\n",
    "general_prompt = PromptTemplate.from_template(\n",
    "    \"You are a helpful assistant. Answer the FAQ question as accurately as you can.\\n\\n{input}\"\n",
    ")\n",
    "\n",
    "# Branching Logic Based on Department\n",
    "prompt_branch = RunnableBranch(\n",
    "    (lambda x: x[\"topic\"] == \"maintenance\", maintenance_prompt),\n",
    "    (lambda x: x[\"topic\"] == \"car_info\", car_info_prompt),\n",
    "    (lambda x: x[\"topic\"] == \"accounts\", accounts_prompt),\n",
    "    general_prompt,\n",
    ")\n",
    "\n",
    "\n",
    "# Topic Classifier for Department Selection\n",
    "class TopicClassifier(BaseModel):\n",
    "    \"Classify the topic of the user question\"\n",
    "    topic: Literal[\"maintenance\", \"car_info\", \"accounts\", \"general\"]\n",
    "    \"The topic of the user question. One of 'maintenance', 'car_info', 'accounts', or 'general'.\"\n",
    "\n",
    "classifier_function = convert_pydantic_to_openai_function(TopicClassifier)\n",
    "llm = get_model().bind(\n",
    "    functions=[classifier_function], function_call={\"name\": \"TopicClassifier\"}\n",
    ")\n",
    "parser = PydanticAttrOutputFunctionsParser(\n",
    "    pydantic_schema=TopicClassifier, attr_name=\"topic\"\n",
    ")\n",
    "classifier_chain = llm | parser\n",
    "\n",
    "    \n",
    "# Final Chain Assembly\n",
    "final_chain = (\n",
    "    RunnablePassthrough.assign(topic=itemgetter(\"input\") | classifier_chain)\n",
    "    | prompt_branch\n",
    "    | get_model()\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example Invocation\n",
    "# final_chain.invoke(\n",
    "    #{\"input\": \"How do I update my payment method for my car subscription?\"}\n",
    "    #{\"input\": \"which is car with the longest range battery?\"}\n",
    "    # {\"input\": \"whats the benefit of a subscription car\"}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d288854",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutputParserException",
     "evalue": "Could not parse function call: 'function_call'\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/openai_functions.py:45\u001b[39m, in \u001b[36mOutputFunctionsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     func_call = copy.deepcopy(\u001b[43mmessage\u001b[49m\u001b[43m.\u001b[49m\u001b[43madditional_kwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[31mKeyError\u001b[39m: 'function_call'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mOutputParserException\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m which_car_maintenance = \u001b[43mfinal_chain\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minput\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mDoes a Toyota Corolla or a Honda Accord need maintenance more frequently?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3055\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3053\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   3054\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3055\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3056\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   3057\u001b[39m         \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/passthrough.py:514\u001b[39m, in \u001b[36mRunnableAssign.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    507\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    509\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    512\u001b[39m     **kwargs: Any,\n\u001b[32m    513\u001b[39m ) -> \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:1936\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1932\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1933\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1934\u001b[39m         output = cast(\n\u001b[32m   1935\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1936\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1938\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1944\u001b[39m         )\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1946\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/passthrough.py:500\u001b[39m, in \u001b[36mRunnableAssign._invoke\u001b[39m\u001b[34m(self, input, run_manager, config, **kwargs)\u001b[39m\n\u001b[32m    495\u001b[39m     msg = \u001b[33m\"\u001b[39m\u001b[33mThe input to RunnablePassthrough.assign() must be a dict.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)  \u001b[38;5;66;03m# noqa: TRY004\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    499\u001b[39m     **\u001b[38;5;28minput\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     **\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpatch_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m    505\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3790\u001b[39m, in \u001b[36mRunnableParallel.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3785\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m   3786\u001b[39m         futures = [\n\u001b[32m   3787\u001b[39m             executor.submit(_invoke_step, step, \u001b[38;5;28minput\u001b[39m, config, key)\n\u001b[32m   3788\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m key, step \u001b[38;5;129;01min\u001b[39;00m steps.items()\n\u001b[32m   3789\u001b[39m         ]\n\u001b[32m-> \u001b[39m\u001b[32m3790\u001b[39m         output = {key: \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m key, future \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(steps, futures)}\n\u001b[32m   3791\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3792\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:456\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    458\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3774\u001b[39m, in \u001b[36mRunnableParallel.invoke.<locals>._invoke_step\u001b[39m\u001b[34m(step, input, config, key)\u001b[39m\n\u001b[32m   3768\u001b[39m child_config = patch_config(\n\u001b[32m   3769\u001b[39m     config,\n\u001b[32m   3770\u001b[39m     \u001b[38;5;66;03m# mark each step as a child run\u001b[39;00m\n\u001b[32m   3771\u001b[39m     callbacks=run_manager.get_child(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mmap:key:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m),\n\u001b[32m   3772\u001b[39m )\n\u001b[32m   3773\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m-> \u001b[39m\u001b[32m3774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3776\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchild_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3778\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:3057\u001b[39m, in \u001b[36mRunnableSequence.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   3055\u001b[39m                 \u001b[38;5;28minput\u001b[39m = context.run(step.invoke, \u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m   3056\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3057\u001b[39m                 \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3058\u001b[39m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[32m   3059\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:92\u001b[39m, in \u001b[36mBaseGenerationOutputParser.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     **kwargs: Any,\n\u001b[32m     90\u001b[39m ) -> T:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrun_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    102\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    103\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    104\u001b[39m             config,\n\u001b[32m    105\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/base.py:1936\u001b[39m, in \u001b[36mRunnable._call_with_config\u001b[39m\u001b[34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[39m\n\u001b[32m   1932\u001b[39m     child_config = patch_config(config, callbacks=run_manager.get_child())\n\u001b[32m   1933\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(child_config) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m   1934\u001b[39m         output = cast(\n\u001b[32m   1935\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOutput\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m-> \u001b[39m\u001b[32m1936\u001b[39m             \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1937\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1938\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1939\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1940\u001b[39m \u001b[43m                \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1941\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1942\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1943\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m   1944\u001b[39m         )\n\u001b[32m   1945\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1946\u001b[39m     run_manager.on_chain_error(e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/runnables/config.py:428\u001b[39m, in \u001b[36mcall_func_with_variable_args\u001b[39m\u001b[34m(func, input, config, run_manager, **kwargs)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[32m    427\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m] = run_manager\n\u001b[32m--> \u001b[39m\u001b[32m428\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/base.py:93\u001b[39m, in \u001b[36mBaseGenerationOutputParser.invoke.<locals>.<lambda>\u001b[39m\u001b[34m(inner_input)\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m     86\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m     **kwargs: Any,\n\u001b[32m     90\u001b[39m ) -> T:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, BaseMessage):\n\u001b[32m     92\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatGeneration\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m=\u001b[49m\u001b[43minner_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     96\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m     97\u001b[39m             config,\n\u001b[32m     98\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     99\u001b[39m         )\n\u001b[32m    100\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    101\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_with_config(\n\u001b[32m    102\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m inner_input: \u001b[38;5;28mself\u001b[39m.parse_result([Generation(text=inner_input)]),\n\u001b[32m    103\u001b[39m             \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    104\u001b[39m             config,\n\u001b[32m    105\u001b[39m             run_type=\u001b[33m\"\u001b[39m\u001b[33mparser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    106\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/openai_functions.py:302\u001b[39m, in \u001b[36mPydanticAttrOutputFunctionsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Any:\n\u001b[32m    293\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m    294\u001b[39m \n\u001b[32m    295\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m    301\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m302\u001b[39m     result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;28mself\u001b[39m.attr_name)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/openai_functions.py:266\u001b[39m, in \u001b[36mPydanticOutputFunctionsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparse_result\u001b[39m(\u001b[38;5;28mself\u001b[39m, result: \u001b[38;5;28mlist\u001b[39m[Generation], *, partial: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m) -> Any:\n\u001b[32m    257\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Parse the result of an LLM call to a JSON object.\u001b[39;00m\n\u001b[32m    258\u001b[39m \n\u001b[32m    259\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    264\u001b[39m \u001b[33;03m        The parsed JSON object.\u001b[39;00m\n\u001b[32m    265\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     _result = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args_only:\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.pydantic_schema, \u001b[33m\"\u001b[39m\u001b[33mmodel_validate_json\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo-clones/ChatGPT-for-Conversational-AI-and-Chatbots/.venv/lib/python3.13/site-packages/langchain_core/output_parsers/openai_functions.py:48\u001b[39m, in \u001b[36mOutputFunctionsParser.parse_result\u001b[39m\u001b[34m(self, result, partial)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     47\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCould not parse function call: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m OutputParserException(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mexc\u001b[39;00m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args_only:\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func_call[\u001b[33m\"\u001b[39m\u001b[33marguments\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[31mOutputParserException\u001b[39m: Could not parse function call: 'function_call'\nFor troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE "
     ]
    }
   ],
   "source": [
    "which_car_maintenance = final_chain.invoke(\n",
    "    {\"input\": \"Does a Toyota Corolla or a Honda Accord need maintenance more frequently?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "48e48510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Toyota Corolla and Honda Accord are known for their reliability, but they have slightly different maintenance schedules:\n",
      "\n",
      "Toyota Corolla:\n",
      "- Typically requires service every 5,000 miles or 6 months\n",
      "- Regular maintenance includes oil changes, tire rotations, and inspections\n",
      "\n",
      "Honda Accord:\n",
      "- Usually needs service every 7,500 miles or 12 months\n",
      "- Regular maintenance includes oil changes, tire rotations, and system checks\n",
      "\n",
      "While the Accord may have slightly longer intervals between services, both cars benefit from following the manufacturer's recommended maintenance schedule. The exact frequency can depend on driving conditions, age of the vehicle, and individual usage.\n",
      "\n",
      "Would you like to book a service appointment for your vehicle? I can help you schedule maintenance with our service center.\n"
     ]
    }
   ],
   "source": [
    "print(which_car_maintenance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
